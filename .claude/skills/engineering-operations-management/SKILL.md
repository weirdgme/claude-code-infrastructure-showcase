# Engineering Operations Management Skill

**For managers running SRE, platform, and infrastructure teams - focusing on operations, on-call, incidents, and engineering metrics.**

> This skill helps engineering managers build sustainable operations practices, prevent burnout, run effective incident reviews, and measure what matters. Complements technical SRE skills with people and process management.

---

## When to Use This Skill

**You're a manager who needs help with:**
- Structuring on-call rotations and compensation
- Preventing and addressing engineer burnout
- Running blameless postmortem reviews
- Negotiating SLOs with product teams
- Measuring engineering productivity (not vanity metrics)
- Managing toil and operational load
- Balancing feature work vs operational excellence
- Building sustainable operations culture

**This skill does NOT cover:**
- Hands-on incident response (see **sre** skill)
- Technical SLO/SLI implementation (see **sre** skill)
- Monitoring/observability setup (see **platform-engineering** skill)
- Team hiring and career development (see **engineering-management** skill)

---

## Core Areas

### 1. On-Call Management

**Core principle:** On-call is a necessary operational burden that should be **fair, sustainable, and compensated**.

#### On-Call Rotation Models

**Follow-the-Sun (Recommended for global teams):**
```
Americas Team:     6 AM - 2 PM PST (primary)
EMEA Team:         2 PM - 10 PM PST (primary)
APAC Team:        10 PM - 6 AM PST (primary)

Pros: No one wakes up at night, balanced load
Cons: Requires global team, handoff complexity
Best for: Teams with 15+ engineers across timezones
```

**Weekly Rotation (Most common):**
```
Week 1: Engineer A (primary), Engineer B (secondary)
Week 2: Engineer C (primary), Engineer D (secondary)
Week 3: Engineer E (primary), Engineer A (secondary)

Pros: Simple, fair rotation
Cons: Weekend coverage, potential burnout
Best for: Teams with 6-10 engineers in same timezone
```

**Tiered Escalation:**
```
Tier 1: Junior engineers (business hours only)
Tier 2: Senior engineers (24/7 primary)
Tier 3: Staff/Principal (escalation only)

Pros: Gradual responsibility increase
Cons: Can create "us vs them" dynamic
Best for: Large teams (15+) with clear skill levels
```

#### On-Call Compensation Models

**Option 1: On-Call Stipend**
```
Primary on-call: $500-$1,000/week
Secondary on-call: $250-$500/week

Pros: Predictable, simple
Cons: Doesn't account for actual pages
```

**Option 2: Pay-per-Page**
```
Business hours page: $50-$100
After-hours page: $150-$300
Weekend/holiday page: $300-$500

Pros: Fair - pays for actual disruption
Cons: Can incentivize ignoring issues
```

**Option 3: Hybrid (Recommended)**
```
Base stipend: $500/week
+ $100 per after-hours page
+ Comp time (1.5x hours worked after-hours)

Pros: Covers both availability and interruptions
Cons: More complex to administer
```

**Comp time policies:**
- For every hour worked after-hours, grant 1.5 hours comp time
- Comp time must be used within 30 days
- Encourage taking comp time day-after major incidents

#### Burnout Prevention

**Warning signs:**
- ğŸš¨ Pages > 5 per week for extended period
- ğŸš¨ Engineer mentions exhaustion, stress in 1-on-1s
- ğŸš¨ Quality of work declining
- ğŸš¨ Working late nights/weekends regularly
- ğŸš¨ Cynicism, disengagement

**Interventions:**
1. **Immediate:** Rotate off on-call for 2-4 weeks
2. **Short-term:** Reduce project load, pair with senior engineer
3. **Long-term:** Fix underlying system issues causing pages

**Sustainable on-call rules:**
- No engineer on-call more than 1 week per month
- Maximum 2 weeks on-call per quarter
- Mandatory break after high-page-volume week
- No on-call during PTO or major life events

---

### 2. Incident Management for Managers

**Your role as a manager during incidents:**

#### During the Incident (DO NOT take over unless critical)

```
âœ… DO:
- Monitor incident channel, offer support
- Shield team from external pressure
- Bring in additional engineers if needed
- Coordinate with stakeholders (updates to execs)
- Order food if it's going long
- Take notes for postmortem

âŒ DON'T:
- Take over incident response (unless you're most qualified)
- Ask "why didn't you..." questions during incident
- Pressure for faster resolution
- Blame individuals
- Second-guess decisions being made
```

**Incident Severity Levels (align with team):**

```
Sev 1 (Critical):
â”œâ”€â”€ Complete service outage
â”œâ”€â”€ Data loss or security breach
â”œâ”€â”€ Revenue impact > $10K/hour
â””â”€â”€ Response: All hands, exec updates every 30 min

Sev 2 (High):
â”œâ”€â”€ Major feature degraded
â”œâ”€â”€ Significant user impact
â”œâ”€â”€ Revenue impact > $1K/hour
â””â”€â”€ Response: On-call + expert, updates every hour

Sev 3 (Medium):
â”œâ”€â”€ Minor feature degraded
â”œâ”€â”€ Limited user impact
â””â”€â”€ Response: On-call handles, regular updates

Sev 4 (Low):
â”œâ”€â”€ Internal tooling issue
â”œâ”€â”€ No user impact
â””â”€â”€ Response: Fix during business hours
```

#### After the Incident: Blameless Postmortem

**Blameless postmortem framework:**

```
Postmortem Template:

## Incident Summary
- Date/Time: When did it happen?
- Duration: How long?
- Impact: Who was affected? How many users?
- Severity: Sev 1-4

## Timeline
- 14:32 - First alert fired
- 14:35 - Engineer A acknowledged, began investigation
- 14:45 - Root cause identified (database connection pool exhausted)
- 15:00 - Mitigation applied (increased pool size)
- 15:15 - Service fully recovered

## Root Cause
What actually caused this? (Technical, not "Engineer X did...")

## What Went Well
- Alert fired within 2 minutes
- Communication was clear
- Rollback was smooth

## What Went Poorly
- No automated mitigation
- Monitoring didn't catch early warning signs
- On-call engineer not familiar with this service

## Action Items
1. [P0] Add automated connection pool scaling (Owner: Alice, Due: 2 weeks)
2. [P1] Improve monitoring for connection pool saturation (Owner: Bob, Due: 1 month)
3. [P2] Add service to on-call training rotation (Owner: Manager, Due: 2 weeks)

## Lessons Learned
- Database connection pool defaults are too conservative
- Need better pre-production load testing
```

**Blameless postmortem meeting (45-60 min):**

```
1. Introduction (5 min)
   â””â”€â”€ Remind: This is blameless, focus on systems not people

2. Timeline Review (15 min)
   â””â”€â”€ Walk through what happened, when

3. Root Cause Analysis (15 min)
   â””â”€â”€ "Why did this happen?" (ask "why" 5 times)

4. What Went Well / What Went Poorly (10 min)
   â””â”€â”€ Balanced reflection

5. Action Items (10 min)
   â””â”€â”€ Specific, assigned, with due dates
   â””â”€â”€ Priority: P0 (this week), P1 (this month), P2 (nice to have)

6. Close (5 min)
   â””â”€â”€ Thank the team, emphasize learning
```

**Red flags in postmortems:**
- âŒ Blaming individuals ("Alice should have...")
- âŒ Vague action items ("Improve monitoring")
- âŒ No follow-up on action items
- âŒ Defensive posturing
- âŒ Skipping postmortems for "small" incidents

**Manager's job:** Enforce blameless culture, track action items, ensure learning.

---

### 3. SLO Negotiation with Product Teams

**The tension:** Product wants features fast. SRE/Platform wants stability. You balance both.

#### Understanding SLOs (Simple Version for Managers)

```
SLI (Service Level Indicator):
What you measure (e.g., "API latency p99")

SLO (Service Level Objective):
Target for reliability (e.g., "API latency p99 < 500ms, 99.9% of the time")

SLA (Service Level Agreement):
Contractual promise to customers (e.g., "99.95% uptime or we give refund")

Example:
SLI: Request success rate
SLO: 99.9% of requests succeed (internal target)
SLA: 99.5% uptime (customer-facing promise)
```

**Error budget concept:**

```
SLO: 99.9% availability = 0.1% allowed downtime

Per month (30 days):
â”œâ”€â”€ Total time: 43,200 minutes
â”œâ”€â”€ Allowed downtime: 43.2 minutes
â””â”€â”€ Error budget: 43.2 minutes

If error budget exhausted:
â”œâ”€â”€ Freeze feature releases
â”œâ”€â”€ Focus on reliability improvements
â””â”€â”€ Pay down tech debt
```

#### SLO Negotiation Framework

**When product pushes for aggressive feature timeline:**

```
Product: "We need to ship this feature in 2 weeks"

You (as manager):
"Let's check our error budget first. If we have budget, we can move fast.
 If we're out of budget, we need to stabilize first."

Scenario 1: Error budget healthy (50% remaining)
â”œâ”€â”€ âœ… Green light for feature work
â”œâ”€â”€ 70% capacity on features
â””â”€â”€ 30% on reliability

Scenario 2: Error budget exhausted (0% remaining)
â”œâ”€â”€ ğŸ›‘ Feature freeze
â”œâ”€â”€ 100% capacity on reliability
â””â”€â”€ Resume features when budget recovers
```

**How to set SLOs (practical guide):**

1. **Start with current performance:**
   - "Our API latency p99 is currently 300ms"
   - Don't set SLO at 300ms - give yourself buffer

2. **Set realistic target:**
   - "Let's set SLO at p99 < 500ms"
   - This gives 200ms buffer for growth/issues

3. **Align with customer expectation:**
   - "Customers complain if latency > 1s"
   - SLO should prevent customer pain

4. **Review quarterly:**
   - Too easy? (Always meeting SLO) â†’ Tighten SLO or invest in features
   - Too hard? (Always missing SLO) â†’ Loosen SLO or invest in reliability

**Common SLOs by service type:**

```
API Services:
â”œâ”€â”€ Availability: 99.9% (43 min downtime/month)
â”œâ”€â”€ Latency p50: < 100ms
â”œâ”€â”€ Latency p99: < 500ms
â””â”€â”€ Error rate: < 0.1%

Batch Processing:
â”œâ”€â”€ Job success rate: 99.5%
â”œâ”€â”€ Job completion time: < 4 hours
â””â”€â”€ Data accuracy: 99.99%

Data Pipeline:
â”œâ”€â”€ Data freshness: < 15 min lag
â”œâ”€â”€ Pipeline availability: 99.9%
â””â”€â”€ Data quality: 99.95%
```

---

### 4. Engineering Metrics That Matter

**The problem:** Easy to measure vanity metrics. Hard to measure real productivity.

#### Vanity Metrics (Avoid)

```
âŒ Lines of code written
âŒ Number of commits
âŒ Hours worked
âŒ Number of deploys (without context)
âŒ Ticket velocity (without quality)
âŒ Code coverage % (without context)
```

**Why these are bad:**
- Lines of code: Good engineers often delete code
- Number of commits: Encourages small, meaningless commits
- Hours worked: Encourages burnout, not productivity
- Deploys without context: Could be hotfixes for bugs you introduced
- Ticket velocity: Encourages cherry-picking easy tickets
- Code coverage: Can write useless tests to hit %

#### Metrics That Actually Matter

**1. DORA Metrics (Use these)**

```
Deployment Frequency:
â”œâ”€â”€ How often do you deploy to production?
â”œâ”€â”€ Elite: Multiple times per day
â”œâ”€â”€ High: Daily to weekly
â”œâ”€â”€ Medium: Weekly to monthly
â””â”€â”€ Low: Monthly to every 6 months

Lead Time for Changes:
â”œâ”€â”€ How long from commit to production?
â”œâ”€â”€ Elite: < 1 hour
â”œâ”€â”€ High: 1 day to 1 week
â”œâ”€â”€ Medium: 1 week to 1 month
â””â”€â”€ Low: 1 month to 6 months

Time to Restore Service:
â”œâ”€â”€ How long to recover from incident?
â”œâ”€â”€ Elite: < 1 hour
â”œâ”€â”€ High: < 1 day
â”œâ”€â”€ Medium: 1 day to 1 week
â””â”€â”€ Low: > 1 week

Change Failure Rate:
â”œâ”€â”€ What % of changes cause incidents?
â”œâ”€â”€ Elite: 0-15%
â”œâ”€â”€ High: 16-30%
â”œâ”€â”€ Medium: 31-45%
â””â”€â”€ Low: > 45%
```

**How to use DORA metrics:**
- Track quarterly, not daily (avoid gaming)
- Trend over time (are we improving?)
- Compare to benchmarks (elite, high, medium, low)
- Use to identify improvement areas

**2. SRE Metrics**

```
Toil Percentage:
â”œâ”€â”€ What % of engineer time is manual ops work?
â”œâ”€â”€ Target: < 30% toil
â”œâ”€â”€ Intervention needed: > 50% toil
â””â”€â”€ Measure: Time tracking, surveys

On-Call Load:
â”œâ”€â”€ Pages per week per engineer
â”œâ”€â”€ Target: < 3 pages/week
â”œâ”€â”€ Intervention: > 5 pages/week
â””â”€â”€ Measure: PagerDuty analytics

SLO Compliance:
â”œâ”€â”€ Are we meeting our SLOs?
â”œâ”€â”€ Target: 99%+ SLO compliance
â””â”€â”€ Measure: Observability dashboards
```

**3. Team Health Metrics**

```
Engineer Satisfaction:
â”œâ”€â”€ Quarterly survey (1-10 scale)
â”œâ”€â”€ Questions: "Satisfied with work?", "Would recommend team?"
â”œâ”€â”€ Target: 8+ average
â””â”€â”€ Red flag: < 6 average or declining trend

Retention Rate:
â”œâ”€â”€ % of engineers staying > 1 year
â”œâ”€â”€ Target: > 85% annual retention
â””â”€â”€ Red flag: < 70% retention

Time to Productivity (New hires):
â”œâ”€â”€ How long until new hire is productive?
â”œâ”€â”€ Target: < 90 days
â””â”€â”€ Measure: Manager assessment + self-assessment
```

**4. Operational Excellence Metrics**

```
Incident Trends:
â”œâ”€â”€ Number of Sev 1/2 incidents per month
â”œâ”€â”€ Target: Declining or stable
â””â”€â”€ Red flag: Increasing trend

Postmortem Action Item Completion:
â”œâ”€â”€ % of action items completed on time
â”œâ”€â”€ Target: > 80% completion
â””â”€â”€ Red flag: < 50% completion

Automated Test Coverage:
â”œâ”€â”€ % of critical paths covered
â”œâ”€â”€ Target: > 70% for critical paths
â””â”€â”€ Not a vanity metric if focused on high-risk areas
```

#### How to Present Metrics to Leadership

**Dashboard structure:**

```
1. Health at a Glance (Top metrics)
   â”œâ”€â”€ ğŸŸ¢ SLO Compliance: 99.8% (Target: 99%)
   â”œâ”€â”€ ğŸŸ¡ Deployment Frequency: 3x/week (Target: Daily)
   â”œâ”€â”€ ğŸŸ¢ Incident Rate: 2 Sev2 this month (Last month: 4)
   â””â”€â”€ ğŸŸ¢ Team Satisfaction: 8.2/10 (Target: 8+)

2. DORA Metrics Trend (Quarterly)
   [Chart showing improvement over time]

3. Focus Areas
   â”œâ”€â”€ âœ… Reduced incident rate by 50% this quarter
   â”œâ”€â”€ ğŸš§ Working on deployment frequency (automation initiative)
   â””â”€â”€ âš ï¸ Toil still high at 40% - hiring 2 more engineers

4. Asks
   â”œâ”€â”€ Budget for observability tooling ($50K)
   â””â”€â”€ Approval to pause feature work next sprint for reliability
```

---

### 5. Balancing Feature Work vs Operational Excellence

**The eternal tension:** Product wants features. You want stability.

#### Resource Allocation Models

**70-20-10 Rule (Recommended):**
```
70% Feature Work:
â”œâ”€â”€ New features product wants
â”œâ”€â”€ Customer-facing improvements
â””â”€â”€ Revenue-generating projects

20% Operational Excellence:
â”œâ”€â”€ Tech debt paydown
â”œâ”€â”€ Reliability improvements
â”œâ”€â”€ Monitoring enhancements
â””â”€â”€ Automation

10% Innovation/Learning:
â”œâ”€â”€ Explore new technologies
â”œâ”€â”€ Hackathons
â”œâ”€â”€ Learning time
â””â”€â”€ Experimentation
```

**Adjust based on phase:**

```
High Growth Phase:
â”œâ”€â”€ 80% Features
â”œâ”€â”€ 15% Ops Excellence
â””â”€â”€ 5% Innovation

Stability Phase:
â”œâ”€â”€ 50% Features
â”œâ”€â”€ 40% Ops Excellence
â””â”€â”€ 10% Innovation

Crisis Phase (Post-Incidents):
â”œâ”€â”€ 30% Features
â”œâ”€â”€ 60% Ops Excellence
â””â”€â”€ 10% Innovation
```

#### Negotiating with Product

**When product asks for all-feature, no-ops time:**

```
Scenario: "We need all engineers on Feature X for Q4"

Your response framework:
1. Acknowledge business need
   "I understand Feature X is critical for revenue"

2. State operational reality
   "Our on-call load is high (8 pages/week) and error budget is 80% exhausted"

3. Present options
   Option A: All-in on features, risk of incidents and burnout
   Option B: 70-30 split, sustainable pace, less feature risk
   Option C: Hire 2 more engineers to do both

4. Recommend
   "I recommend Option B - we'll deliver 70% of Feature X this quarter,
    and ensure we don't have outages that impact customers"

5. Make it their decision
   "What's your preference given these trade-offs?"
```

**Using error budgets as negotiation tool:**

```
Error budget = objective metric, not subjective

If product wants to move fast:
â”œâ”€â”€ Check error budget: 50% remaining?
â”œâ”€â”€ âœ… Green light: "We have budget, let's ship!"
â””â”€â”€ ğŸ›‘ Budget exhausted: "We need to stabilize first"

This removes emotion from discussion. It's data-driven.
```

---

### 6. Building Sustainable Operations Culture

**Culture eats process for breakfast.**

#### Key Cultural Values

**1. Blameless Culture**
```
When incidents happen:
âŒ "Who broke it?" â†’ âœ… "What broke?"
âŒ "Why didn't you..." â†’ âœ… "What can we learn?"
âŒ Hide mistakes â†’ âœ… Share failures openly
```

**2. Automate Toil**
```
Manual work is not a badge of honor.
â”œâ”€â”€ Track toil percentage
â”œâ”€â”€ Reward automation, not heroics
â””â”€â”€ "If you do it twice, automate it"
```

**3. Sustainable On-Call**
```
On-call is not punishment.
â”œâ”€â”€ Fair rotation
â”œâ”€â”€ Compensated fairly
â”œâ”€â”€ Protected from burnout
â””â”€â”€ Escalation is encouraged, not weakness
```

**4. Continuous Improvement**
```
Every incident is a learning opportunity.
â”œâ”€â”€ Postmortems are required, not optional
â”œâ”€â”€ Action items are tracked and completed
â””â”€â”€ Celebrate fixes, not just features
```

#### Manager Actions to Reinforce Culture

**1. Lead by Example**
- Participate in on-call rotation (if you're technical)
- Admit your own mistakes publicly
- Take postmortem action items yourself

**2. Celebrate Operational Wins**
- Shout out engineers who reduce toil
- Highlight reliability improvements in team meetings
- Give "Operational Excellence" awards

**3. Protect Your Team**
- Say no to unrealistic timelines
- Push back on "just ship it" pressure
- Shield team from org politics

**4. Invest in Automation**
- Allocate 20% capacity to ops excellence
- Approve tool/platform budgets
- Hire for automation skills

---

## Quick Reference for Managers

**On-Call:**
- Rotation: Weekly or follow-the-sun
- Compensation: $500-$1000/week + pay-per-page
- Burnout prevention: Max 1 week/month, comp time after incidents

**Incidents:**
- Your role: Support, don't take over
- Blameless postmortems: Required for all Sev 1/2
- Action items: Track and ensure completion

**SLOs:**
- Start with current performance + buffer
- Use error budgets to negotiate with product
- Review quarterly

**Metrics:**
- Use DORA metrics (deployment freq, lead time, MTTR, change failure rate)
- Avoid vanity metrics (lines of code, commits, hours)
- Track team health (satisfaction, retention)

**Resource Allocation:**
- 70% features, 20% ops excellence, 10% innovation
- Adjust based on phase (growth vs stability)

**Culture:**
- Blameless, automate toil, sustainable on-call
- Lead by example, celebrate ops wins
- Protect team from burnout

---

## Integration with Other Skills

**This skill works with:**
- **engineering-management** - Hiring, career development, 1-on-1s
- **technical-leadership** - Making technical decisions, risk assessment
- **infrastructure-strategy** - Long-term planning, platform investment
- **budget-and-cost-management** - On-call budgets, tooling costs

**Technical skills your team uses:**
- **sre** - Hands-on SLO implementation, incident response
- **platform-engineering** - Building internal platforms that reduce toil
- **cybersecurity** - Security incident response, compliance

---

**Remember:** Your job is to build sustainable operations practices that enable long-term success, not short-term heroics. Protect your team from burnout. Measure what matters. Learn from every incident.
